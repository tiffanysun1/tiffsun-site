<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
  <title>Tiffany Sun</title>
</head>
<body>
  <!-- header background for scrolled state -->
  <div class="header-bg"></div>
  
  <!-- hero -->
  <section id="hero" class="panel hero">
    <div class="intro-text">hello world, i'm tiffany sun</div>
    <a href="#about" class="scroll-down" aria-label="scroll down">↓</a>
  </section>

  <!-- nav -->
  <nav class="nav-links">
    <a href="#about">about</a>
    <a href="#projects">projects</a>
    <a href="#art">art</a>
    <a href="#blog">blog</a>
  </nav>

  <main>
    <section id="about"    class="panel content-section about-me">
      <h2>about me</h2>
      <p>hi! i recently graduated uc berkeley with a computer science degree, now working as a software engineer at <a href="https://www.atlassian.com/">atlassian</a>.</p>
      <p>connect with me:</p>
      <li>
        <a href="https://linkedin.com/in/tiffanysun17" target="_blank" rel="noopener noreferrer">linkedin</a>
      </li>
      <li>
        <a href="https://github.com/tiffanysun1" target="_blank" rel="noopener noreferrer">github</a>
      </li>
      <li>
        <a href="https://twitter.com/tiffnami" target="_blank" rel="noopener noreferrer">twitter</a>
      </li>
    </section>

    <section id="projects" class="panel content-section">
      <h2>projects</h2>
      <br>
      <h3>chibify</h3>
      <p>an app that converts your photo into your chibi version</p>
      <div class="embeds-container">
          <!-- Twitter/X Embed -->
          <div class="embed-item">
            <div class="twitter-wrapper">
              <blockquote class="twitter-tweet">
                <p lang="en" dir="ltr"><a href="https://twitter.com/tiffnami/status/1808036557598908688?ref_src=twsrc%5Etfw">June 29, 2024</a></p>&mdash; tiffany (@tiffnami) <a href="https://twitter.com/tiffnami/status/1808036557598908688?ref_src=twsrc%5Etfw">June 29, 2024</a>
              </blockquote>
            </div>
          </div>
      </div>
      <br>
      <h3>aiart</h3>
        <p>a space to train art styles and generate art</p>
        <div class="embeds-container">
        <div class="embed-item">
            <div class="embed-wrapper">
                <iframe src="https://www.loom.com/embed/dbb2dbe3f81d4554a2acba00266c9531?sid=255d6d3d-605d-4f61-9c3c-609b86c92e63" 
                frameborder="0" 
                webkitallowfullscreen 
                mozallowfullscreen 
                allowfullscreen>
                </iframe>   
            </div>
        </div>
        </div>
        <br>
        <h3>autochapter</h3>
        <p>automatically chapter lecture videos, generate chapter summaries, and search for specific chapters</p>
        <div class="embeds-container">
            <div class="embed-item">
                <div class="embed-wrapper">
                    <img src="autochapter.png" alt="Autochapter project screenshot" style="width:100%; height:100%; object-fit:contain; position:absolute;">
                </div>
            </div>
        </div>
    </section>

    <section id="art"      class="panel content-section">
        <h2>art</h2>
        <p>some of my art</p>
      <div class="art-gallery">
        <div class="art-item">
          <div class="art-wrapper">
            <img src="honey.jpg" alt="Honey artwork" class="protected-image">
            <div class="image-overlay"></div>
          </div>
          <p class="art-title">honey</p>
        </div>
        <!-- <div class="art-item">
          <div class="art-wrapper">
            <img src="ice-cream.jpg" alt="Ice cream artwork" class="protected-image">
            <div class="image-overlay"></div>
          </div>
          <p class="art-title">ice cream</p>
        </div> -->
        <div class="art-item">
          <div class="art-wrapper">
            <img src="traffic.jpg" alt="Traffic artwork" class="protected-image">
            <div class="image-overlay"></div>
          </div>
          <p class="art-title">traffic</p>
        </div>
      </div>
    </section>

    <section id="blog"     class="panel content-section">
      <h2>blog</h2>
      
      <article class="blog-post">
        <h3>blog #1</h3>
        
        <p>Fix the bug, run the commands, approve the change.<br>
        SWE-Bench measures the first, Terminal-Bench the second, but we still lack a standard benchmark for the third.</p>
        
        <p>So I built a lightweight benchmark to test 5 major AI models on real pull request decisions from major open-source projects like Kubernetes and VS Code, with real outcomes.</p>
        
        <p>The setup was intentionally simple:</p>
        <ul>
          <li>20 real PRs (10 approve, 10 reject)</li>
          <li>5 AI agents/models: Claude-code, CodexCLI (o3-mini), OpenCode (o3), AMP (o4-mini), and Gemini-CLI</li>
          <li>Approve or Reject</li>
          <li>Same evaluation criteria for everyone</li>
        </ul>
        
        <h4>Results</h4>
        <table>
          <thead>
            <tr>
              <th>Model</th>
              <th>Overall Accuracy</th>
              <th>Approval Rate</th>
              <th>Quality Score</th>
            </tr>
          </thead>
          <tbody>
            <tr><td>OpenCode</td><td>70%</td><td>80%</td><td>0.89</td></tr>
            <tr><td>CodexCLI</td><td>60%</td><td>90%</td><td>0.64</td></tr>
            <tr><td>Gemini-CLI</td><td>60%</td><td>50%</td><td>0.83</td></tr>
            <tr><td>Claude-Code</td><td>60%</td><td>90%</td><td>0.68</td></tr>
            <tr><td>AMP</td><td>55%</td><td>85%</td><td>0.71</td></tr>
          </tbody>
        </table>
        
        <p>The overlapping pattern was that most models were way too nice. CodexCLI and Claude-Code approved 90% of PRs, including the ones that were clearly incomplete or problematic.</p>
        
        <p>Here are the false positive rates:</p>
        <p>CodexCLI, Claude-Code, AMP: 80%<br>
        OpenCode: 60%<br>
        Gemini-CLI: 60%</p>
        
        <p>Only Gemini-CLI achieved a balanced 50% approval rate, closest to the ground truth distribution. It was also the only one that wrongly rejected good PRs (40% false negative rate).</p>
        
        <h4>What This Means</h4>
        
        <p>Most of the AI Models are "Yes Men". They are conflict-averse reviewers who would rather say yes than risk blocking a good change. If you're using AI for code review, weight their rejections more heavily than approvals.</p>
        
        <p>OpenCode wins here, but not just because of accuracy:</p>
        <ul>
          <li>Highest accuracy (70%) AND highest quality reasoning (0.89/1.0)</li>
          <li>Provided specific technical analysis: "This PR improves code consistency by using native pyarrow grouped aggregation functions..."</li>
          <li>Referenced actual PR content and implementation details</li>
        </ul>
        
        <p>Compare that to CodexCLI's typical response: "The changes look good and address the issue effectively."</p>
        
        <p>Gemini-CLI showed the most balanced judgment but sometimes got bogged down in edge cases that didn't matter.</p>
        
        <h4>Dataset Issues:</h4>
        <ul>
          <li>PR #2: Kubernetes docs fix that added an unnecessary "ACKNOWLEGEMENT" section with typos</li>
          <li>PR #4: VS Code readonly files feature—complex implementation, mixed reviews</li>
          <li>PR #15: Rust doctest optimization—significant performance improvement but breaking change concerns</li>
        </ul>
        
        <p>With only 20 examples, each mistake carries huge weight (5% accuracy hit). The models might perform differently on:</p>
        <ul>
          <li>Different project types (web vs systems vs data)</li>
          <li>Larger codebases with more context</li>
          <li>PRs with CI/CD results and test outputs</li>
        </ul>
        
        <h4>Agent vs Model: A Critical Distinction</h4>
        
        <p>Here's something important I realized: I was testing raw language models, not coding agents. What I actually tested was models getting a text prompt with PR info. They didn't have the ability to:</p>
        <ul>
          <li>run code or tests</li>
          <li>browse the repo or docs</li>
          <li>view CI/CD results or commit history</li>
        </ul>
        
        <p>A real coding agent should be able to do all of that, iterate on its work, and ask clarifying questions. This explains some of the approval bias—without being able to verify claims, models default to trusting the PR author.</p>
        
        <h4>What's Next?</h4>
        
        <p>This experiment scratched the surface. To really understand AI code review capabilities, we need more including but not limited to:</p>
        <ul>
          <li>a larger dataset across different language/project types</li>
          <li>real agent testing</li>
          <li>better prompt engineering</li>
          <li>few-shot examples of good reviews</li>
        </ul>
        
        <p>Beyond benchmarks, there's promising potential in customizing coding agents to match an organization's specific coding standards by synthesizing targeted training data from historical PRs and reviews. This aligns closely with OpenAI's own strategy of using AI models to improve data preparation and training processes for other AI systems, creating a self-reinforcing loop of continuous improvement.</p>
        
        <h4>Related Work:</h4>
        
        <p>There's a related benchmark here, which attempts a similar goal but has notable limitations: it focuses primarily on simple, synthetic scenarios, missing the nuance and complexity found in real, large-scale open-source projects. My benchmark aims explicitly to capture authentic code-review dynamics.</p>
        
        <p>If you'd like to dive deeper or explore the data and code behind my study, you can visit my full repository:</p>
        
        <p><strong>➡️ <a href="#" target="_blank" rel="noopener noreferrer">pr-agents-benchmark</a></strong></p>
      </article>
    </section>
    <br>
  </main>

  <script>
    /* toggle state once hero leaves viewport */
    const hero = document.getElementById('hero');
    const observer = new IntersectionObserver(([e]) =>
      document.body.classList.toggle('scrolled', !e.isIntersecting),
      {threshold: 1}
    );
    observer.observe(hero);

    /* smooth scrolling only for anchor links (starting with #) */
    document.querySelectorAll('a[href^="#"]').forEach(a =>
      a.addEventListener('click', e => {
        e.preventDefault();
        document.querySelector(a.getAttribute('href'))
                .scrollIntoView({behavior:'smooth'});
      })
    );

    /* manually trigger twitter widget loading after page loads */
    window.addEventListener('load', () => {
      if (window.twttr && window.twttr.widgets) {
        window.twttr.widgets.load();
      }
    });

    /* disable right-click and other context menus on art images */
    document.querySelectorAll('.art-wrapper').forEach(wrapper => {
      wrapper.addEventListener('contextmenu', e => e.preventDefault());
      wrapper.addEventListener('dragstart', e => e.preventDefault());
      wrapper.addEventListener('selectstart', e => e.preventDefault());
    });
  </script>
  
  <!-- Twitter widgets script -->
  <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</body>
</html>